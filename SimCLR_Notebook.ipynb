{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimCLR Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9461f27e96e54e0e8ee3fe1c9f883100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d52b9c0c196e46e7aee403df08137b58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e34806633ee541e099d169e3414753ec",
              "IPY_MODEL_c79b0c7d04ea4275ab433a5ebaa5fb1b"
            ]
          }
        },
        "d52b9c0c196e46e7aee403df08137b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e34806633ee541e099d169e3414753ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9fdc910e3c9f406fa883f1db38e436f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2640397119,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2640397119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab96d530f34f460a91dae16a92431e69"
          }
        },
        "c79b0c7d04ea4275ab433a5ebaa5fb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c0e917e8fc641ec8f4854d0d1476dc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640397312/? [05:11&lt;00:00, 8468354.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43cdba5ca4614c91a108e318885a8de3"
          }
        },
        "9fdc910e3c9f406fa883f1db38e436f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab96d530f34f460a91dae16a92431e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c0e917e8fc641ec8f4854d0d1476dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43cdba5ca4614c91a108e318885a8de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SGSunil/simclr/blob/master/SimCLR_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8ZoVrwt0n0"
      },
      "source": [
        "# SimCLR\n",
        "PyTorch implementation of SimCLR: A Simple Framework for Contrastive Learning of Visual Representations by T. Chen et al. With support for the LARS (Layer-wise Adaptive Rate Scaling) optimizer and global batch norm.\n",
        "\n",
        "[Link to paper](https://arxiv.org/pdf/2002.05709.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6WMxjCvN3o"
      },
      "source": [
        "## Setup the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53JMIYtat8tT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafd6adc-fb20-4bc3-dc21-574e32fcb038"
      },
      "source": [
        "!git clone https://github.com/spijkervet/SimCLR.git\n",
        "%cd SimCLR\n",
        "!mkdir -p logs && cd logs && wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar && cd ../\n",
        "!wget https://raw.githubusercontent.com/Spijkervet/SimCLR/master/requirements.txt\n",
        "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
        "!pip install  pyyaml --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SimCLR'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 531 (delta 0), reused 0 (delta 0), pack-reused 527\u001b[K\n",
            "Receiving objects: 100% (531/531), 327.97 KiB | 10.93 MiB/s, done.\n",
            "Resolving deltas: 100% (292/292), done.\n",
            "/content/SimCLR/SimCLR\n",
            "--2021-07-25 11:07:48--  https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210725T110711Z&X-Amz-Expires=300&X-Amz-Signature=e06d4a21060b20f87ec05ba9fb5e6513bc29a5b69f29ff5f5599bc9db96fc7c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-25 11:07:48--  https://github-releases.githubusercontent.com/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210725T110711Z&X-Amz-Expires=300&X-Amz-Signature=e06d4a21060b20f87ec05ba9fb5e6513bc29a5b69f29ff5f5599bc9db96fc7c5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.111.154, 185.199.108.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111607632 (106M) [application/octet-stream]\n",
            "Saving to: â€˜checkpoint_100.tarâ€™\n",
            "\n",
            "checkpoint_100.tar  100%[===================>] 106.44M   204MB/s    in 0.5s    \n",
            "\n",
            "2021-07-25 11:07:49 (204 MB/s) - â€˜checkpoint_100.tarâ€™ saved [111607632/111607632]\n",
            "\n",
            "--2021-07-25 11:07:49--  https://raw.githubusercontent.com/Spijkervet/SimCLR/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24 [text/plain]\n",
            "Saving to: â€˜requirements.txt.1â€™\n",
            "\n",
            "requirements.txt.1  100%[===================>]      24  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-25 11:07:49 (1.82 MB/s) - â€˜requirements.txt.1â€™ saved [24/24]\n",
            "\n",
            "setup.sh: 2: setup.sh: conda: not found\n",
            "setup.sh: 2: setup.sh: conda: not found\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.10.0+cu102)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (5.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ3jq3cWynLf"
      },
      "source": [
        "# Part 1:\n",
        "## SimCLR pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jhAv3hv8IHn"
      },
      "source": [
        "# whether to use a TPU or not (set in Runtime -> Change Runtime Type)\n",
        "use_tpu = False\n",
        "!mkdir save"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwW10d2O7pn8"
      },
      "source": [
        "#### Install PyTorch/XLA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj84aiC27oxS"
      },
      "source": [
        "if use_tpu:\n",
        "  VERSION = \"20200220\" #@param [\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNDRcPbbymlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20908a4-500c-4043-98bf-1e791c2fffb2"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if use_tpu:\n",
        "  # imports the torch_xla package for TPU support\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  dev = xm.xla_device()\n",
        "  print(dev)\n",
        "  \n",
        "import torchvision\n",
        "import argparse\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "apex = False\n",
        "try:\n",
        "    from apex import amp\n",
        "    apex = True\n",
        "except ImportError:\n",
        "    print(\n",
        "        \"Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\"\n",
        "    )\n",
        "\n",
        "from model import save_model, load_optimizer\n",
        "from simclr import SimCLR\n",
        "from simclr.modules import get_resnet, NT_Xent\n",
        "from simclr.modules.transformations import TransformsSimCLR"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbV0fa_y03Z"
      },
      "source": [
        "### Load arguments from `config/config.yaml`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1klUf-IuyxdL"
      },
      "source": [
        "from pprint import pprint\n",
        "import argparse\n",
        "from utils import yaml_config_hook\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "for k, v in config.items():\n",
        "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
        "\n",
        "args = parser.parse_args([])\n",
        "args.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l0T8k4XXONa",
        "outputId": "5e89f872-8621-46c0-ff43-263a1cf32511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "args.device"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O86__UhA0Lvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fb4d87-5d02-4dd6-c993-782cd2ec3627"
      },
      "source": [
        "### override any configuration parameters here, e.g. to adjust for use on GPUs on the Colab platform:\n",
        "args.batch_size = 128\n",
        "args.resnet = \"resnet18\"\n",
        "pprint(vars(args))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 128,\n",
            " 'dataparallel': 0,\n",
            " 'dataset': 'CIFAR10',\n",
            " 'dataset_dir': './datasets',\n",
            " 'device': device(type='cuda'),\n",
            " 'epoch_num': 100,\n",
            " 'epochs': 100,\n",
            " 'gpus': 1,\n",
            " 'image_size': 224,\n",
            " 'logistic_batch_size': 256,\n",
            " 'logistic_epochs': 500,\n",
            " 'model_path': 'save',\n",
            " 'nodes': 1,\n",
            " 'nr': 0,\n",
            " 'optimizer': 'Adam',\n",
            " 'pretrain': True,\n",
            " 'projection_dim': 64,\n",
            " 'reload': False,\n",
            " 'resnet': 'resnet18',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJfeOM9PzNoF"
      },
      "source": [
        "### Load dataset into train loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGcskdBsytbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8364c87f-3cfc-4fb4-c252-bba0693ff20a"
      },
      "source": [
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "if args.dataset == \"STL10\":\n",
        "    train_dataset = torchvision.datasets.STL10(\n",
        "        args.dataset_dir,\n",
        "        split=\"unlabeled\",\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size),\n",
        "    )\n",
        "elif args.dataset == \"CIFAR10\":\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        args.dataset_dir,\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size),\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "if args.nodes > 1:\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=True\n",
        "    )\n",
        "else:\n",
        "    train_sampler = None\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=(train_sampler is None),\n",
        "    drop_last=True,\n",
        "    num_workers=args.workers,\n",
        "    sampler=train_sampler,\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlXZwvjzPmp"
      },
      "source": [
        "### Load the SimCLR model, optimizer and learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xERq_yHSzJRX"
      },
      "source": [
        "# initialize ResNet\n",
        "encoder = get_resnet(args.resnet, pretrained=False)\n",
        "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
        "\n",
        "# initialize model\n",
        "model = SimCLR(encoder, args.projection_dim, n_features)\n",
        "if args.reload:\n",
        "    model_fp = os.path.join(\n",
        "        args.model_path, \"checkpoint_{}.tar\".format(args.epoch_num)\n",
        "    )\n",
        "    model.load_state_dict(torch.load(model_fp, map_location=args.device.type))\n",
        "model = model.to(args.device)\n",
        "\n",
        "# optimizer / loss\n",
        "optimizer, scheduler = load_optimizer(args, model)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtNCVEynzjtV"
      },
      "source": [
        "### Initialize the criterion (NT-Xent loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u067AY93zh-k"
      },
      "source": [
        "criterion = NT_Xent(args.batch_size, args.temperature, world_size=1)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyJ3ulWqzViL"
      },
      "source": [
        "### Setup TensorBoard for logging experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZNieMqfzU7H"
      },
      "source": [
        "writer = SummaryWriter()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXMOVfg47Hlh"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyMFhpB-7HGj"
      },
      "source": [
        "def train(args, train_loader, model, criterion, optimizer, writer):\n",
        "    loss_epoch = 0\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x_i = x_i.cuda(non_blocking=True)\n",
        "        x_j = x_j.cuda(non_blocking=True)\n",
        "\n",
        "        # positive pair, with encoding\n",
        "        h_i, h_j, z_i, z_j = model(x_i, x_j)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"step: \", step)\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
        "        loss_epoch += loss.item()\n",
        "        args.global_step += 1\n",
        "    return loss_epoch\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5KBK-yztGD"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdCrD62hzjDQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04d2eab3-9e7b-4ab5-d3e4-8575d6606839"
      },
      "source": [
        "args.global_step = 0\n",
        "args.current_epoch = 0\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    # save every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        save_model(args, model, optimizer)\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\n",
        "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
        "    print(\n",
        "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
        "    )\n",
        "    args.current_epoch += 1\n",
        "\n",
        "# end training\n",
        "save_model(args, model, optimizer)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step:  0\n",
            "Step [0/390]\t Loss: 5.5189290046691895\n",
            "step:  1\n",
            "step:  2\n",
            "step:  3\n",
            "step:  4\n",
            "step:  5\n",
            "step:  6\n",
            "step:  7\n",
            "step:  8\n",
            "step:  9\n",
            "step:  10\n",
            "step:  11\n",
            "step:  12\n",
            "step:  13\n",
            "step:  14\n",
            "step:  15\n",
            "step:  16\n",
            "step:  17\n",
            "step:  18\n",
            "step:  19\n",
            "step:  20\n",
            "step:  21\n",
            "step:  22\n",
            "step:  23\n",
            "step:  24\n",
            "step:  25\n",
            "step:  26\n",
            "step:  27\n",
            "step:  28\n",
            "step:  29\n",
            "step:  30\n",
            "step:  31\n",
            "step:  32\n",
            "step:  33\n",
            "step:  34\n",
            "step:  35\n",
            "step:  36\n",
            "step:  37\n",
            "step:  38\n",
            "step:  39\n",
            "step:  40\n",
            "step:  41\n",
            "step:  42\n",
            "step:  43\n",
            "step:  44\n",
            "step:  45\n",
            "step:  46\n",
            "step:  47\n",
            "step:  48\n",
            "step:  49\n",
            "step:  50\n",
            "Step [50/390]\t Loss: 5.371453762054443\n",
            "step:  51\n",
            "step:  52\n",
            "step:  53\n",
            "step:  54\n",
            "step:  55\n",
            "step:  56\n",
            "step:  57\n",
            "step:  58\n",
            "step:  59\n",
            "step:  60\n",
            "step:  61\n",
            "step:  62\n",
            "step:  63\n",
            "step:  64\n",
            "step:  65\n",
            "step:  66\n",
            "step:  67\n",
            "step:  68\n",
            "step:  69\n",
            "step:  70\n",
            "step:  71\n",
            "step:  72\n",
            "step:  73\n",
            "step:  74\n",
            "step:  75\n",
            "step:  76\n",
            "step:  77\n",
            "step:  78\n",
            "step:  79\n",
            "step:  80\n",
            "step:  81\n",
            "step:  82\n",
            "step:  83\n",
            "step:  84\n",
            "step:  85\n",
            "step:  86\n",
            "step:  87\n",
            "step:  88\n",
            "step:  89\n",
            "step:  90\n",
            "step:  91\n",
            "step:  92\n",
            "step:  93\n",
            "step:  94\n",
            "step:  95\n",
            "step:  96\n",
            "step:  97\n",
            "step:  98\n",
            "step:  99\n",
            "step:  100\n",
            "Step [100/390]\t Loss: 5.055074214935303\n",
            "step:  101\n",
            "step:  102\n",
            "step:  103\n",
            "step:  104\n",
            "step:  105\n",
            "step:  106\n",
            "step:  107\n",
            "step:  108\n",
            "step:  109\n",
            "step:  110\n",
            "step:  111\n",
            "step:  112\n",
            "step:  113\n",
            "step:  114\n",
            "step:  115\n",
            "step:  116\n",
            "step:  117\n",
            "step:  118\n",
            "step:  119\n",
            "step:  120\n",
            "step:  121\n",
            "step:  122\n",
            "step:  123\n",
            "step:  124\n",
            "step:  125\n",
            "step:  126\n",
            "step:  127\n",
            "step:  128\n",
            "step:  129\n",
            "step:  130\n",
            "step:  131\n",
            "step:  132\n",
            "step:  133\n",
            "step:  134\n",
            "step:  135\n",
            "step:  136\n",
            "step:  137\n",
            "step:  138\n",
            "step:  139\n",
            "step:  140\n",
            "step:  141\n",
            "step:  142\n",
            "step:  143\n",
            "step:  144\n",
            "step:  145\n",
            "step:  146\n",
            "step:  147\n",
            "step:  148\n",
            "step:  149\n",
            "step:  150\n",
            "Step [150/390]\t Loss: 5.176442623138428\n",
            "step:  151\n",
            "step:  152\n",
            "step:  153\n",
            "step:  154\n",
            "step:  155\n",
            "step:  156\n",
            "step:  157\n",
            "step:  158\n",
            "step:  159\n",
            "step:  160\n",
            "step:  161\n",
            "step:  162\n",
            "step:  163\n",
            "step:  164\n",
            "step:  165\n",
            "step:  166\n",
            "step:  167\n",
            "step:  168\n",
            "step:  169\n",
            "step:  170\n",
            "step:  171\n",
            "step:  172\n",
            "step:  173\n",
            "step:  174\n",
            "step:  175\n",
            "step:  176\n",
            "step:  177\n",
            "step:  178\n",
            "step:  179\n",
            "step:  180\n",
            "step:  181\n",
            "step:  182\n",
            "step:  183\n",
            "step:  184\n",
            "step:  185\n",
            "step:  186\n",
            "step:  187\n",
            "step:  188\n",
            "step:  189\n",
            "step:  190\n",
            "step:  191\n",
            "step:  192\n",
            "step:  193\n",
            "step:  194\n",
            "step:  195\n",
            "step:  196\n",
            "step:  197\n",
            "step:  198\n",
            "step:  199\n",
            "step:  200\n",
            "Step [200/390]\t Loss: 4.988724708557129\n",
            "step:  201\n",
            "step:  202\n",
            "step:  203\n",
            "step:  204\n",
            "step:  205\n",
            "step:  206\n",
            "step:  207\n",
            "step:  208\n",
            "step:  209\n",
            "step:  210\n",
            "step:  211\n",
            "step:  212\n",
            "step:  213\n",
            "step:  214\n",
            "step:  215\n",
            "step:  216\n",
            "step:  217\n",
            "step:  218\n",
            "step:  219\n",
            "step:  220\n",
            "step:  221\n",
            "step:  222\n",
            "step:  223\n",
            "step:  224\n",
            "step:  225\n",
            "step:  226\n",
            "step:  227\n",
            "step:  228\n",
            "step:  229\n",
            "step:  230\n",
            "step:  231\n",
            "step:  232\n",
            "step:  233\n",
            "step:  234\n",
            "step:  235\n",
            "step:  236\n",
            "step:  237\n",
            "step:  238\n",
            "step:  239\n",
            "step:  240\n",
            "step:  241\n",
            "step:  242\n",
            "step:  243\n",
            "step:  244\n",
            "step:  245\n",
            "step:  246\n",
            "step:  247\n",
            "step:  248\n",
            "step:  249\n",
            "step:  250\n",
            "Step [250/390]\t Loss: 5.05690336227417\n",
            "step:  251\n",
            "step:  252\n",
            "step:  253\n",
            "step:  254\n",
            "step:  255\n",
            "step:  256\n",
            "step:  257\n",
            "step:  258\n",
            "step:  259\n",
            "step:  260\n",
            "step:  261\n",
            "step:  262\n",
            "step:  263\n",
            "step:  264\n",
            "step:  265\n",
            "step:  266\n",
            "step:  267\n",
            "step:  268\n",
            "step:  269\n",
            "step:  270\n",
            "step:  271\n",
            "step:  272\n",
            "step:  273\n",
            "step:  274\n",
            "step:  275\n",
            "step:  276\n",
            "step:  277\n",
            "step:  278\n",
            "step:  279\n",
            "step:  280\n",
            "step:  281\n",
            "step:  282\n",
            "step:  283\n",
            "step:  284\n",
            "step:  285\n",
            "step:  286\n",
            "step:  287\n",
            "step:  288\n",
            "step:  289\n",
            "step:  290\n",
            "step:  291\n",
            "step:  292\n",
            "step:  293\n",
            "step:  294\n",
            "step:  295\n",
            "step:  296\n",
            "step:  297\n",
            "step:  298\n",
            "step:  299\n",
            "step:  300\n",
            "Step [300/390]\t Loss: 5.068678379058838\n",
            "step:  301\n",
            "step:  302\n",
            "step:  303\n",
            "step:  304\n",
            "step:  305\n",
            "step:  306\n",
            "step:  307\n",
            "step:  308\n",
            "step:  309\n",
            "step:  310\n",
            "step:  311\n",
            "step:  312\n",
            "step:  313\n",
            "step:  314\n",
            "step:  315\n",
            "step:  316\n",
            "step:  317\n",
            "step:  318\n",
            "step:  319\n",
            "step:  320\n",
            "step:  321\n",
            "step:  322\n",
            "step:  323\n",
            "step:  324\n",
            "step:  325\n",
            "step:  326\n",
            "step:  327\n",
            "step:  328\n",
            "step:  329\n",
            "step:  330\n",
            "step:  331\n",
            "step:  332\n",
            "step:  333\n",
            "step:  334\n",
            "step:  335\n",
            "step:  336\n",
            "step:  337\n",
            "step:  338\n",
            "step:  339\n",
            "step:  340\n",
            "step:  341\n",
            "step:  342\n",
            "step:  343\n",
            "step:  344\n",
            "step:  345\n",
            "step:  346\n",
            "step:  347\n",
            "step:  348\n",
            "step:  349\n",
            "step:  350\n",
            "Step [350/390]\t Loss: 5.0974931716918945\n",
            "step:  351\n",
            "step:  352\n",
            "step:  353\n",
            "step:  354\n",
            "step:  355\n",
            "step:  356\n",
            "step:  357\n",
            "step:  358\n",
            "step:  359\n",
            "step:  360\n",
            "step:  361\n",
            "step:  362\n",
            "step:  363\n",
            "step:  364\n",
            "step:  365\n",
            "step:  366\n",
            "step:  367\n",
            "step:  368\n",
            "step:  369\n",
            "step:  370\n",
            "step:  371\n",
            "step:  372\n",
            "step:  373\n",
            "step:  374\n",
            "step:  375\n",
            "step:  376\n",
            "step:  377\n",
            "step:  378\n",
            "step:  379\n",
            "step:  380\n",
            "step:  381\n",
            "step:  382\n",
            "step:  383\n",
            "step:  384\n",
            "step:  385\n",
            "step:  386\n",
            "step:  387\n",
            "step:  388\n",
            "step:  389\n",
            "Epoch [0/100]\t Loss: 5.140452611140716\t lr: 0.0003\n",
            "step:  0\n",
            "Step [0/390]\t Loss: 5.031724452972412\n",
            "step:  1\n",
            "step:  2\n",
            "step:  3\n",
            "step:  4\n",
            "step:  5\n",
            "step:  6\n",
            "step:  7\n",
            "step:  8\n",
            "step:  9\n",
            "step:  10\n",
            "step:  11\n",
            "step:  12\n",
            "step:  13\n",
            "step:  14\n",
            "step:  15\n",
            "step:  16\n",
            "step:  17\n",
            "step:  18\n",
            "step:  19\n",
            "step:  20\n",
            "step:  21\n",
            "step:  22\n",
            "step:  23\n",
            "step:  24\n",
            "step:  25\n",
            "step:  26\n",
            "step:  27\n",
            "step:  28\n",
            "step:  29\n",
            "step:  30\n",
            "step:  31\n",
            "step:  32\n",
            "step:  33\n",
            "step:  34\n",
            "step:  35\n",
            "step:  36\n",
            "step:  37\n",
            "step:  38\n",
            "step:  39\n",
            "step:  40\n",
            "step:  41\n",
            "step:  42\n",
            "step:  43\n",
            "step:  44\n",
            "step:  45\n",
            "step:  46\n",
            "step:  47\n",
            "step:  48\n",
            "step:  49\n",
            "step:  50\n",
            "Step [50/390]\t Loss: 4.9089155197143555\n",
            "step:  51\n",
            "step:  52\n",
            "step:  53\n",
            "step:  54\n",
            "step:  55\n",
            "step:  56\n",
            "step:  57\n",
            "step:  58\n",
            "step:  59\n",
            "step:  60\n",
            "step:  61\n",
            "step:  62\n",
            "step:  63\n",
            "step:  64\n",
            "step:  65\n",
            "step:  66\n",
            "step:  67\n",
            "step:  68\n",
            "step:  69\n",
            "step:  70\n",
            "step:  71\n",
            "step:  72\n",
            "step:  73\n",
            "step:  74\n",
            "step:  75\n",
            "step:  76\n",
            "step:  77\n",
            "step:  78\n",
            "step:  79\n",
            "step:  80\n",
            "step:  81\n",
            "step:  82\n",
            "step:  83\n",
            "step:  84\n",
            "step:  85\n",
            "step:  86\n",
            "step:  87\n",
            "step:  88\n",
            "step:  89\n",
            "step:  90\n",
            "step:  91\n",
            "step:  92\n",
            "step:  93\n",
            "step:  94\n",
            "step:  95\n",
            "step:  96\n",
            "step:  97\n",
            "step:  98\n",
            "step:  99\n",
            "step:  100\n",
            "Step [100/390]\t Loss: 4.768424987792969\n",
            "step:  101\n",
            "step:  102\n",
            "step:  103\n",
            "step:  104\n",
            "step:  105\n",
            "step:  106\n",
            "step:  107\n",
            "step:  108\n",
            "step:  109\n",
            "step:  110\n",
            "step:  111\n",
            "step:  112\n",
            "step:  113\n",
            "step:  114\n",
            "step:  115\n",
            "step:  116\n",
            "step:  117\n",
            "step:  118\n",
            "step:  119\n",
            "step:  120\n",
            "step:  121\n",
            "step:  122\n",
            "step:  123\n",
            "step:  124\n",
            "step:  125\n",
            "step:  126\n",
            "step:  127\n",
            "step:  128\n",
            "step:  129\n",
            "step:  130\n",
            "step:  131\n",
            "step:  132\n",
            "step:  133\n",
            "step:  134\n",
            "step:  135\n",
            "step:  136\n",
            "step:  137\n",
            "step:  138\n",
            "step:  139\n",
            "step:  140\n",
            "step:  141\n",
            "step:  142\n",
            "step:  143\n",
            "step:  144\n",
            "step:  145\n",
            "step:  146\n",
            "step:  147\n",
            "step:  148\n",
            "step:  149\n",
            "step:  150\n",
            "Step [150/390]\t Loss: 4.812432289123535\n",
            "step:  151\n",
            "step:  152\n",
            "step:  153\n",
            "step:  154\n",
            "step:  155\n",
            "step:  156\n",
            "step:  157\n",
            "step:  158\n",
            "step:  159\n",
            "step:  160\n",
            "step:  161\n",
            "step:  162\n",
            "step:  163\n",
            "step:  164\n",
            "step:  165\n",
            "step:  166\n",
            "step:  167\n",
            "step:  168\n",
            "step:  169\n",
            "step:  170\n",
            "step:  171\n",
            "step:  172\n",
            "step:  173\n",
            "step:  174\n",
            "step:  175\n",
            "step:  176\n",
            "step:  177\n",
            "step:  178\n",
            "step:  179\n",
            "step:  180\n",
            "step:  181\n",
            "step:  182\n",
            "step:  183\n",
            "step:  184\n",
            "step:  185\n",
            "step:  186\n",
            "step:  187\n",
            "step:  188\n",
            "step:  189\n",
            "step:  190\n",
            "step:  191\n",
            "step:  192\n",
            "step:  193\n",
            "step:  194\n",
            "step:  195\n",
            "step:  196\n",
            "step:  197\n",
            "step:  198\n",
            "step:  199\n",
            "step:  200\n",
            "Step [200/390]\t Loss: 4.802513599395752\n",
            "step:  201\n",
            "step:  202\n",
            "step:  203\n",
            "step:  204\n",
            "step:  205\n",
            "step:  206\n",
            "step:  207\n",
            "step:  208\n",
            "step:  209\n",
            "step:  210\n",
            "step:  211\n",
            "step:  212\n",
            "step:  213\n",
            "step:  214\n",
            "step:  215\n",
            "step:  216\n",
            "step:  217\n",
            "step:  218\n",
            "step:  219\n",
            "step:  220\n",
            "step:  221\n",
            "step:  222\n",
            "step:  223\n",
            "step:  224\n",
            "step:  225\n",
            "step:  226\n",
            "step:  227\n",
            "step:  228\n",
            "step:  229\n",
            "step:  230\n",
            "step:  231\n",
            "step:  232\n",
            "step:  233\n",
            "step:  234\n",
            "step:  235\n",
            "step:  236\n",
            "step:  237\n",
            "step:  238\n",
            "step:  239\n",
            "step:  240\n",
            "step:  241\n",
            "step:  242\n",
            "step:  243\n",
            "step:  244\n",
            "step:  245\n",
            "step:  246\n",
            "step:  247\n",
            "step:  248\n",
            "step:  249\n",
            "step:  250\n",
            "Step [250/390]\t Loss: 4.7493486404418945\n",
            "step:  251\n",
            "step:  252\n",
            "step:  253\n",
            "step:  254\n",
            "step:  255\n",
            "step:  256\n",
            "step:  257\n",
            "step:  258\n",
            "step:  259\n",
            "step:  260\n",
            "step:  261\n",
            "step:  262\n",
            "step:  263\n",
            "step:  264\n",
            "step:  265\n",
            "step:  266\n",
            "step:  267\n",
            "step:  268\n",
            "step:  269\n",
            "step:  270\n",
            "step:  271\n",
            "step:  272\n",
            "step:  273\n",
            "step:  274\n",
            "step:  275\n",
            "step:  276\n",
            "step:  277\n",
            "step:  278\n",
            "step:  279\n",
            "step:  280\n",
            "step:  281\n",
            "step:  282\n",
            "step:  283\n",
            "step:  284\n",
            "step:  285\n",
            "step:  286\n",
            "step:  287\n",
            "step:  288\n",
            "step:  289\n",
            "step:  290\n",
            "step:  291\n",
            "step:  292\n",
            "step:  293\n",
            "step:  294\n",
            "step:  295\n",
            "step:  296\n",
            "step:  297\n",
            "step:  298\n",
            "step:  299\n",
            "step:  300\n",
            "Step [300/390]\t Loss: 4.639235019683838\n",
            "step:  301\n",
            "step:  302\n",
            "step:  303\n",
            "step:  304\n",
            "step:  305\n",
            "step:  306\n",
            "step:  307\n",
            "step:  308\n",
            "step:  309\n",
            "step:  310\n",
            "step:  311\n",
            "step:  312\n",
            "step:  313\n",
            "step:  314\n",
            "step:  315\n",
            "step:  316\n",
            "step:  317\n",
            "step:  318\n",
            "step:  319\n",
            "step:  320\n",
            "step:  321\n",
            "step:  322\n",
            "step:  323\n",
            "step:  324\n",
            "step:  325\n",
            "step:  326\n",
            "step:  327\n",
            "step:  328\n",
            "step:  329\n",
            "step:  330\n",
            "step:  331\n",
            "step:  332\n",
            "step:  333\n",
            "step:  334\n",
            "step:  335\n",
            "step:  336\n",
            "step:  337\n",
            "step:  338\n",
            "step:  339\n",
            "step:  340\n",
            "step:  341\n",
            "step:  342\n",
            "step:  343\n",
            "step:  344\n",
            "step:  345\n",
            "step:  346\n",
            "step:  347\n",
            "step:  348\n",
            "step:  349\n",
            "step:  350\n",
            "Step [350/390]\t Loss: 4.7588300704956055\n",
            "step:  351\n",
            "step:  352\n",
            "step:  353\n",
            "step:  354\n",
            "step:  355\n",
            "step:  356\n",
            "step:  357\n",
            "step:  358\n",
            "step:  359\n",
            "step:  360\n",
            "step:  361\n",
            "step:  362\n",
            "step:  363\n",
            "step:  364\n",
            "step:  365\n",
            "step:  366\n",
            "step:  367\n",
            "step:  368\n",
            "step:  369\n",
            "step:  370\n",
            "step:  371\n",
            "step:  372\n",
            "step:  373\n",
            "step:  374\n",
            "step:  375\n",
            "step:  376\n",
            "step:  377\n",
            "step:  378\n",
            "step:  379\n",
            "step:  380\n",
            "step:  381\n",
            "step:  382\n",
            "step:  383\n",
            "step:  384\n",
            "step:  385\n",
            "step:  386\n",
            "step:  387\n",
            "step:  388\n",
            "step:  389\n",
            "Epoch [1/100]\t Loss: 4.800521916609544\t lr: 0.0003\n",
            "step:  0\n",
            "Step [0/390]\t Loss: 4.616102695465088\n",
            "step:  1\n",
            "step:  2\n",
            "step:  3\n",
            "step:  4\n",
            "step:  5\n",
            "step:  6\n",
            "step:  7\n",
            "step:  8\n",
            "step:  9\n",
            "step:  10\n",
            "step:  11\n",
            "step:  12\n",
            "step:  13\n",
            "step:  14\n",
            "step:  15\n",
            "step:  16\n",
            "step:  17\n",
            "step:  18\n",
            "step:  19\n",
            "step:  20\n",
            "step:  21\n",
            "step:  22\n",
            "step:  23\n",
            "step:  24\n",
            "step:  25\n",
            "step:  26\n",
            "step:  27\n",
            "step:  28\n",
            "step:  29\n",
            "step:  30\n",
            "step:  31\n",
            "step:  32\n",
            "step:  33\n",
            "step:  34\n",
            "step:  35\n",
            "step:  36\n",
            "step:  37\n",
            "step:  38\n",
            "step:  39\n",
            "step:  40\n",
            "step:  41\n",
            "step:  42\n",
            "step:  43\n",
            "step:  44\n",
            "step:  45\n",
            "step:  46\n",
            "step:  47\n",
            "step:  48\n",
            "step:  49\n",
            "step:  50\n",
            "Step [50/390]\t Loss: 4.532656669616699\n",
            "step:  51\n",
            "step:  52\n",
            "step:  53\n",
            "step:  54\n",
            "step:  55\n",
            "step:  56\n",
            "step:  57\n",
            "step:  58\n",
            "step:  59\n",
            "step:  60\n",
            "step:  61\n",
            "step:  62\n",
            "step:  63\n",
            "step:  64\n",
            "step:  65\n",
            "step:  66\n",
            "step:  67\n",
            "step:  68\n",
            "step:  69\n",
            "step:  70\n",
            "step:  71\n",
            "step:  72\n",
            "step:  73\n",
            "step:  74\n",
            "step:  75\n",
            "step:  76\n",
            "step:  77\n",
            "step:  78\n",
            "step:  79\n",
            "step:  80\n",
            "step:  81\n",
            "step:  82\n",
            "step:  83\n",
            "step:  84\n",
            "step:  85\n",
            "step:  86\n",
            "step:  87\n",
            "step:  88\n",
            "step:  89\n",
            "step:  90\n",
            "step:  91\n",
            "step:  92\n",
            "step:  93\n",
            "step:  94\n",
            "step:  95\n",
            "step:  96\n",
            "step:  97\n",
            "step:  98\n",
            "step:  99\n",
            "step:  100\n",
            "Step [100/390]\t Loss: 4.648200988769531\n",
            "step:  101\n",
            "step:  102\n",
            "step:  103\n",
            "step:  104\n",
            "step:  105\n",
            "step:  106\n",
            "step:  107\n",
            "step:  108\n",
            "step:  109\n",
            "step:  110\n",
            "step:  111\n",
            "step:  112\n",
            "step:  113\n",
            "step:  114\n",
            "step:  115\n",
            "step:  116\n",
            "step:  117\n",
            "step:  118\n",
            "step:  119\n",
            "step:  120\n",
            "step:  121\n",
            "step:  122\n",
            "step:  123\n",
            "step:  124\n",
            "step:  125\n",
            "step:  126\n",
            "step:  127\n",
            "step:  128\n",
            "step:  129\n",
            "step:  130\n",
            "step:  131\n",
            "step:  132\n",
            "step:  133\n",
            "step:  134\n",
            "step:  135\n",
            "step:  136\n",
            "step:  137\n",
            "step:  138\n",
            "step:  139\n",
            "step:  140\n",
            "step:  141\n",
            "step:  142\n",
            "step:  143\n",
            "step:  144\n",
            "step:  145\n",
            "step:  146\n",
            "step:  147\n",
            "step:  148\n",
            "step:  149\n",
            "step:  150\n",
            "Step [150/390]\t Loss: 4.578997611999512\n",
            "step:  151\n",
            "step:  152\n",
            "step:  153\n",
            "step:  154\n",
            "step:  155\n",
            "step:  156\n",
            "step:  157\n",
            "step:  158\n",
            "step:  159\n",
            "step:  160\n",
            "step:  161\n",
            "step:  162\n",
            "step:  163\n",
            "step:  164\n",
            "step:  165\n",
            "step:  166\n",
            "step:  167\n",
            "step:  168\n",
            "step:  169\n",
            "step:  170\n",
            "step:  171\n",
            "step:  172\n",
            "step:  173\n",
            "step:  174\n",
            "step:  175\n",
            "step:  176\n",
            "step:  177\n",
            "step:  178\n",
            "step:  179\n",
            "step:  180\n",
            "step:  181\n",
            "step:  182\n",
            "step:  183\n",
            "step:  184\n",
            "step:  185\n",
            "step:  186\n",
            "step:  187\n",
            "step:  188\n",
            "step:  189\n",
            "step:  190\n",
            "step:  191\n",
            "step:  192\n",
            "step:  193\n",
            "step:  194\n",
            "step:  195\n",
            "step:  196\n",
            "step:  197\n",
            "step:  198\n",
            "step:  199\n",
            "step:  200\n",
            "Step [200/390]\t Loss: 4.565555572509766\n",
            "step:  201\n",
            "step:  202\n",
            "step:  203\n",
            "step:  204\n",
            "step:  205\n",
            "step:  206\n",
            "step:  207\n",
            "step:  208\n",
            "step:  209\n",
            "step:  210\n",
            "step:  211\n",
            "step:  212\n",
            "step:  213\n",
            "step:  214\n",
            "step:  215\n",
            "step:  216\n",
            "step:  217\n",
            "step:  218\n",
            "step:  219\n",
            "step:  220\n",
            "step:  221\n",
            "step:  222\n",
            "step:  223\n",
            "step:  224\n",
            "step:  225\n",
            "step:  226\n",
            "step:  227\n",
            "step:  228\n",
            "step:  229\n",
            "step:  230\n",
            "step:  231\n",
            "step:  232\n",
            "step:  233\n",
            "step:  234\n",
            "step:  235\n",
            "step:  236\n",
            "step:  237\n",
            "step:  238\n",
            "step:  239\n",
            "step:  240\n",
            "step:  241\n",
            "step:  242\n",
            "step:  243\n",
            "step:  244\n",
            "step:  245\n",
            "step:  246\n",
            "step:  247\n",
            "step:  248\n",
            "step:  249\n",
            "step:  250\n",
            "Step [250/390]\t Loss: 4.559428691864014\n",
            "step:  251\n",
            "step:  252\n",
            "step:  253\n",
            "step:  254\n",
            "step:  255\n",
            "step:  256\n",
            "step:  257\n",
            "step:  258\n",
            "step:  259\n",
            "step:  260\n",
            "step:  261\n",
            "step:  262\n",
            "step:  263\n",
            "step:  264\n",
            "step:  265\n",
            "step:  266\n",
            "step:  267\n",
            "step:  268\n",
            "step:  269\n",
            "step:  270\n",
            "step:  271\n",
            "step:  272\n",
            "step:  273\n",
            "step:  274\n",
            "step:  275\n",
            "step:  276\n",
            "step:  277\n",
            "step:  278\n",
            "step:  279\n",
            "step:  280\n",
            "step:  281\n",
            "step:  282\n",
            "step:  283\n",
            "step:  284\n",
            "step:  285\n",
            "step:  286\n",
            "step:  287\n",
            "step:  288\n",
            "step:  289\n",
            "step:  290\n",
            "step:  291\n",
            "step:  292\n",
            "step:  293\n",
            "step:  294\n",
            "step:  295\n",
            "step:  296\n",
            "step:  297\n",
            "step:  298\n",
            "step:  299\n",
            "step:  300\n",
            "Step [300/390]\t Loss: 4.600470066070557\n",
            "step:  301\n",
            "step:  302\n",
            "step:  303\n",
            "step:  304\n",
            "step:  305\n",
            "step:  306\n",
            "step:  307\n",
            "step:  308\n",
            "step:  309\n",
            "step:  310\n",
            "step:  311\n",
            "step:  312\n",
            "step:  313\n",
            "step:  314\n",
            "step:  315\n",
            "step:  316\n",
            "step:  317\n",
            "step:  318\n",
            "step:  319\n",
            "step:  320\n",
            "step:  321\n",
            "step:  322\n",
            "step:  323\n",
            "step:  324\n",
            "step:  325\n",
            "step:  326\n",
            "step:  327\n",
            "step:  328\n",
            "step:  329\n",
            "step:  330\n",
            "step:  331\n",
            "step:  332\n",
            "step:  333\n",
            "step:  334\n",
            "step:  335\n",
            "step:  336\n",
            "step:  337\n",
            "step:  338\n",
            "step:  339\n",
            "step:  340\n",
            "step:  341\n",
            "step:  342\n",
            "step:  343\n",
            "step:  344\n",
            "step:  345\n",
            "step:  346\n",
            "step:  347\n",
            "step:  348\n",
            "step:  349\n",
            "step:  350\n",
            "Step [350/390]\t Loss: 4.553007125854492\n",
            "step:  351\n",
            "step:  352\n",
            "step:  353\n",
            "step:  354\n",
            "step:  355\n",
            "step:  356\n",
            "step:  357\n",
            "step:  358\n",
            "step:  359\n",
            "step:  360\n",
            "step:  361\n",
            "step:  362\n",
            "step:  363\n",
            "step:  364\n",
            "step:  365\n",
            "step:  366\n",
            "step:  367\n",
            "step:  368\n",
            "step:  369\n",
            "step:  370\n",
            "step:  371\n",
            "step:  372\n",
            "step:  373\n",
            "step:  374\n",
            "step:  375\n",
            "step:  376\n",
            "step:  377\n",
            "step:  378\n",
            "step:  379\n",
            "step:  380\n",
            "step:  381\n",
            "step:  382\n",
            "step:  383\n",
            "step:  384\n",
            "step:  385\n",
            "step:  386\n",
            "step:  387\n",
            "step:  388\n",
            "step:  389\n",
            "Epoch [2/100]\t Loss: 4.593661923286242\t lr: 0.0003\n",
            "step:  0\n",
            "Step [0/390]\t Loss: 4.549920082092285\n",
            "step:  1\n",
            "step:  2\n",
            "step:  3\n",
            "step:  4\n",
            "step:  5\n",
            "step:  6\n",
            "step:  7\n",
            "step:  8\n",
            "step:  9\n",
            "step:  10\n",
            "step:  11\n",
            "step:  12\n",
            "step:  13\n",
            "step:  14\n",
            "step:  15\n",
            "step:  16\n",
            "step:  17\n",
            "step:  18\n",
            "step:  19\n",
            "step:  20\n",
            "step:  21\n",
            "step:  22\n",
            "step:  23\n",
            "step:  24\n",
            "step:  25\n",
            "step:  26\n",
            "step:  27\n",
            "step:  28\n",
            "step:  29\n",
            "step:  30\n",
            "step:  31\n",
            "step:  32\n",
            "step:  33\n",
            "step:  34\n",
            "step:  35\n",
            "step:  36\n",
            "step:  37\n",
            "step:  38\n",
            "step:  39\n",
            "step:  40\n",
            "step:  41\n",
            "step:  42\n",
            "step:  43\n",
            "step:  44\n",
            "step:  45\n",
            "step:  46\n",
            "step:  47\n",
            "step:  48\n",
            "step:  49\n",
            "step:  50\n",
            "Step [50/390]\t Loss: 4.503045082092285\n",
            "step:  51\n",
            "step:  52\n",
            "step:  53\n",
            "step:  54\n",
            "step:  55\n",
            "step:  56\n",
            "step:  57\n",
            "step:  58\n",
            "step:  59\n",
            "step:  60\n",
            "step:  61\n",
            "step:  62\n",
            "step:  63\n",
            "step:  64\n",
            "step:  65\n",
            "step:  66\n",
            "step:  67\n",
            "step:  68\n",
            "step:  69\n",
            "step:  70\n",
            "step:  71\n",
            "step:  72\n",
            "step:  73\n",
            "step:  74\n",
            "step:  75\n",
            "step:  76\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-f75b52b629a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-f4fb20ae27e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_loader, model, criterion, optimizer, writer)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/train_epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77BXUR9_4hNc"
      },
      "source": [
        "## OPTIONAL: Download last checkpoint to local drive (replace `100` with `args.epochs`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7eHATk04Sgu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "3544f463-4dfd-470a-fc48-28238207d8b2"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint_100.tar')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-ba3b665a570a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_100.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: checkpoint_100.tar"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQpjiuJy61N"
      },
      "source": [
        "# Part 2:\n",
        "## Linear evaluation using logistic regression, using weights from frozen, pre-trained SimCLR model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24wrzMP2vYcV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFyS9RvpuCuC"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import argparse\n",
        "from simclr.modules import LogisticRegression\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZRtPBCLvgqz"
      },
      "source": [
        "def train(args, loader, simclr_model, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    accuracy_epoch = 0\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        predicted = output.argmax(1)\n",
        "        acc = (predicted == y).sum().item() / y.size(0)\n",
        "        accuracy_epoch += acc\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        # if step % 100 == 0:\n",
        "        #     print(\n",
        "        #         f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()}\\t Accuracy: {acc}\"\n",
        "        #     )\n",
        "\n",
        "    return loss_epoch, accuracy_epoch"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBYAPb2uKB5"
      },
      "source": [
        "def test(args, loader, simclr_model, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    accuracy_epoch = 0\n",
        "    model.eval()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        model.zero_grad()\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        predicted = output.argmax(1)\n",
        "        acc = (predicted == y).sum().item() / y.size(0)\n",
        "        accuracy_epoch += acc\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "\n",
        "    return loss_epoch, accuracy_epoch\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJk4-nc-vkF0"
      },
      "source": [
        "from pprint import pprint\n",
        "from utils import yaml_config_hook\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "for k, v in config.items():\n",
        "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "if use_tpu:\n",
        "  args.device = dev\n",
        "else:\n",
        "  args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7cSwhu55KJc"
      },
      "source": [
        "args.batch_size = 64\n",
        "args.dataset = \"STL10\" # make sure to check this with the (pre-)trained checkpoint\n",
        "args.resnet = \"resnet50\" # make sure to check this with the (pre-)trained checkpoint\n",
        "args.model_path = \"logs\"\n",
        "args.epoch_num = 100\n",
        "args.logistic_epochs = 500"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTgCE-mZ7ygx"
      },
      "source": [
        "### Download a pre-trained model for demonstration purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMuPgP3h7vfi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9755cfd9-e4db-4214-a32a-7f8cc449f486"
      },
      "source": [
        "!wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-13 20:47:57--  https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200713T204757Z&X-Amz-Expires=300&X-Amz-Signature=66ef1af62e159b36feeb4a5199ed257f16a98294da54b7ce2b06d84389026c56&X-Amz-SignedHeaders=host&actor_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-07-13 20:47:58--  https://github-production-release-asset-2e65be.s3.amazonaws.com/246276098/8ae3c180-64bd-11ea-91fe-0f47017fe9be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200713T204757Z&X-Amz-Expires=300&X-Amz-Signature=66ef1af62e159b36feeb4a5199ed257f16a98294da54b7ce2b06d84389026c56&X-Amz-SignedHeaders=host&actor_id=0&repo_id=246276098&response-content-disposition=attachment%3B%20filename%3Dcheckpoint_100.tar&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.111.3\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.111.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111607632 (106M) [application/octet-stream]\n",
            "Saving to: â€˜checkpoint_100.tarâ€™\n",
            "\n",
            "checkpoint_100.tar  100%[===================>] 106.44M  16.4MB/s    in 7.9s    \n",
            "\n",
            "2020-07-13 20:48:06 (13.6 MB/s) - â€˜checkpoint_100.tarâ€™ saved [111607632/111607632]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRuVrZZ5Vm1"
      },
      "source": [
        "### Load dataset into train/test dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGuFjLW5PF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "9461f27e96e54e0e8ee3fe1c9f883100",
            "d52b9c0c196e46e7aee403df08137b58",
            "e34806633ee541e099d169e3414753ec",
            "c79b0c7d04ea4275ab433a5ebaa5fb1b",
            "9fdc910e3c9f406fa883f1db38e436f0",
            "ab96d530f34f460a91dae16a92431e69",
            "1c0e917e8fc641ec8f4854d0d1476dc4",
            "43cdba5ca4614c91a108e318885a8de3"
          ]
        },
        "outputId": "b5538e28-4a73-4466-950d-1e23e4ad4a8b"
      },
      "source": [
        "if args.dataset == \"STL10\":\n",
        "    train_dataset = torchvision.datasets.STL10(\n",
        "        args.dataset_dir,\n",
        "        split=\"train\",\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.STL10(\n",
        "        args.dataset_dir,\n",
        "        split=\"test\",\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
        "    )\n",
        "elif args.dataset == \"CIFAR10\":\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        args.dataset_dir,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        args.dataset_dir,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.logistic_batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=args.workers,\n",
        ")\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./datasets/stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9461f27e96e54e0e8ee3fe1c9f883100",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2640397119.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./datasets/stl10_binary.tar.gz to ./datasets\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI4pRNpdfZRA",
        "outputId": "693d8bd3-4369-4290-ce72-993f9ebe9684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.logistic_batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=args.workers,\n",
        ")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwXqVBH5ZX6"
      },
      "source": [
        "### Load ResNet encoder / SimCLR and load model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTVnvx2a5QnX"
      },
      "source": [
        "encoder = get_resnet(args.resnet, pretrained=False) # don't load a pre-trained model from PyTorch repo\n",
        "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
        "\n",
        "# load pre-trained model from checkpoint\n",
        "simclr_model = SimCLR( encoder, args.projection_dim, n_features)\n",
        "model_fp = os.path.join(\n",
        "    \"logs\", \"checkpoint_{}.tar\".format(args.epoch_num)\n",
        ")\n",
        "simclr_model.load_state_dict(torch.load(model_fp, map_location=args.device.type))\n",
        "simclr_model = simclr_model.to(args.device)\n",
        "    "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-zyvEjGuD8d",
        "outputId": "1edbbb3a-4149-4a14-b5cf-4752ffc95eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "args.device.type"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSSuW0cMjXM3",
        "outputId": "9007e4d8-fded-4550-cde2-a450a9177ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "args.device.type"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIWlB901dw2e",
        "outputId": "1be1b70d-2dfe-4bad-8ee4-28274689a39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "args.device"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krA8eDT3dwNC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZoABGRr5Q8_"
      },
      "source": [
        "## Logistic Regression\n",
        "n_classes = 10 # stl-10 / cifar-10\n",
        "model = LogisticRegression(simclr_model.n_features, n_classes)\n",
        "model = model.to(args.device)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T694n_HQ5Tad"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLgDCu1uTLQ5"
      },
      "source": [
        "### Helper functions to map all input data $X$ to their latent representations $h$ that are used in linear evaluation (they only have to be computed once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B6li5NVSWR3"
      },
      "source": [
        "def inference(loader, simclr_model, device):\n",
        "    feature_vector = []\n",
        "    labels_vector = []\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "\n",
        "        # get encoding\n",
        "        with torch.no_grad():\n",
        "            h, _, z, _ = simclr_model(x, x)\n",
        "\n",
        "        h = h.detach()\n",
        "\n",
        "        feature_vector.extend(h.cpu().detach().numpy())\n",
        "        labels_vector.extend(y.numpy())\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
        "\n",
        "    feature_vector = np.array(feature_vector)\n",
        "    labels_vector = np.array(labels_vector)\n",
        "    print(\"Features shape {}\".format(feature_vector.shape))\n",
        "    return feature_vector, labels_vector\n",
        "\n",
        "\n",
        "def get_features(context_model, train_loader, test_loader, device):\n",
        "    train_X, train_y = inference(train_loader, context_model, device)\n",
        "    test_X, test_y = inference(test_loader, context_model, device)\n",
        "    return train_X, train_y, test_X, test_y\n",
        "\n",
        "\n",
        "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test, batch_size):\n",
        "    train = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train, batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    test = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test, batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPeoK6ZkS4MB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b93f375-4f57-4ea0-cf24-03dbbc7b7355"
      },
      "source": [
        "print(\"### Creating features from pre-trained context model ###\")\n",
        "(train_X, train_y, test_X, test_y) = get_features(\n",
        "    simclr_model, train_loader, test_loader, args.device\n",
        ")\n",
        "\n",
        "arr_train_loader, arr_test_loader = create_data_loaders_from_arrays(\n",
        "    train_X, train_y, test_X, test_y, args.logistic_batch_size\n",
        ")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Creating features from pre-trained context model ###\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step [0/19]\t Computing features...\n",
            "Features shape (4864, 2048)\n",
            "Step [0/31]\t Computing features...\n",
            "Step [20/31]\t Computing features...\n",
            "Features shape (7936, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLaebM9Qvztx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5787169b-333a-4183-a4ff-2b77d8f65d6f"
      },
      "source": [
        "for epoch in range(args.logistic_epochs):\n",
        "    loss_epoch, accuracy_epoch = train(args, arr_train_loader, simclr_model, model, criterion, optimizer)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Epoch [{epoch}/{args.logistic_epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t Accuracy: {accuracy_epoch / len(train_loader)}\")\n",
        "\n",
        "\n",
        "# final testing\n",
        "loss_epoch, accuracy_epoch = test(\n",
        "    args, arr_test_loader, simclr_model, model, criterion, optimizer\n",
        ")\n",
        "print(\n",
        "    f\"[FINAL]\\t Loss: {loss_epoch / len(test_loader)}\\t Accuracy: {accuracy_epoch / len(test_loader)}\"\n",
        ")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/500]\t Loss: 1.3823571612960415\t Accuracy: 0.5575657894736842\n",
            "Epoch [10/500]\t Loss: 0.5902091625489687\t Accuracy: 0.7888569078947368\n",
            "Epoch [20/500]\t Loss: 0.545104128749747\t Accuracy: 0.8040707236842105\n",
            "Epoch [30/500]\t Loss: 0.5162652919166967\t Accuracy: 0.8141447368421053\n",
            "Epoch [40/500]\t Loss: 0.4943905378642835\t Accuracy: 0.8196957236842105\n",
            "Epoch [50/500]\t Loss: 0.4765512786413494\t Accuracy: 0.826891447368421\n",
            "Epoch [60/500]\t Loss: 0.46139819998490184\t Accuracy: 0.8330592105263158\n",
            "Epoch [70/500]\t Loss: 0.4481848243035768\t Accuracy: 0.8384046052631579\n",
            "Epoch [80/500]\t Loss: 0.43644678592681885\t Accuracy: 0.8429276315789473\n",
            "Epoch [90/500]\t Loss: 0.4258712718361302\t Accuracy: 0.84765625\n",
            "Epoch [100/500]\t Loss: 0.4162356100584331\t Accuracy: 0.8507401315789473\n",
            "Epoch [110/500]\t Loss: 0.4073750972747803\t Accuracy: 0.8544407894736842\n",
            "Epoch [120/500]\t Loss: 0.39916436295760305\t Accuracy: 0.858141447368421\n",
            "Epoch [130/500]\t Loss: 0.391505944101434\t Accuracy: 0.86328125\n",
            "Epoch [140/500]\t Loss: 0.384322768763492\t Accuracy: 0.8678042763157895\n",
            "Epoch [150/500]\t Loss: 0.37755292810891805\t Accuracy: 0.8708881578947368\n",
            "Epoch [160/500]\t Loss: 0.3711462914943695\t Accuracy: 0.8729440789473685\n",
            "Epoch [170/500]\t Loss: 0.365061679953023\t Accuracy: 0.8760279605263158\n",
            "Epoch [180/500]\t Loss: 0.3592651251115297\t Accuracy: 0.8784950657894737\n",
            "Epoch [190/500]\t Loss: 0.3537282677073228\t Accuracy: 0.8803453947368421\n",
            "Epoch [200/500]\t Loss: 0.3484273129387906\t Accuracy: 0.8828125\n",
            "Epoch [210/500]\t Loss: 0.34334204698863785\t Accuracy: 0.8848684210526315\n",
            "Epoch [220/500]\t Loss: 0.3384551970582259\t Accuracy: 0.8863075657894737\n",
            "Epoch [230/500]\t Loss: 0.33375181336151927\t Accuracy: 0.8881578947368421\n",
            "Epoch [240/500]\t Loss: 0.3292188111104463\t Accuracy: 0.8904194078947368\n",
            "Epoch [250/500]\t Loss: 0.32484469131419536\t Accuracy: 0.8930921052631579\n",
            "Epoch [260/500]\t Loss: 0.3206191455063067\t Accuracy: 0.89453125\n",
            "Epoch [270/500]\t Loss: 0.3165329788860522\t Accuracy: 0.8959703947368421\n",
            "Epoch [280/500]\t Loss: 0.3125778483717065\t Accuracy: 0.8969983552631579\n",
            "Epoch [290/500]\t Loss: 0.30874612456873846\t Accuracy: 0.8984375\n",
            "Epoch [300/500]\t Loss: 0.30503083059662267\t Accuracy: 0.8998766447368421\n",
            "Epoch [310/500]\t Loss: 0.3014254428838429\t Accuracy: 0.9009046052631579\n",
            "Epoch [320/500]\t Loss: 0.2979239837119454\t Accuracy: 0.9013157894736842\n",
            "Epoch [330/500]\t Loss: 0.2945208478915064\t Accuracy: 0.9031661184210527\n",
            "Epoch [340/500]\t Loss: 0.2912108121733916\t Accuracy: 0.9048108552631579\n",
            "Epoch [350/500]\t Loss: 0.28798901485769374\t Accuracy: 0.9060444078947368\n",
            "Epoch [360/500]\t Loss: 0.2848509448139291\t Accuracy: 0.907483552631579\n",
            "Epoch [370/500]\t Loss: 0.2817923865820232\t Accuracy: 0.9087171052631579\n",
            "Epoch [380/500]\t Loss: 0.2788094156666806\t Accuracy: 0.9107730263157895\n",
            "Epoch [390/500]\t Loss: 0.27589845657348633\t Accuracy: 0.9124177631578947\n",
            "Epoch [400/500]\t Loss: 0.27305608752526733\t Accuracy: 0.9140625\n",
            "Epoch [410/500]\t Loss: 0.27027922241311325\t Accuracy: 0.915296052631579\n",
            "Epoch [420/500]\t Loss: 0.2675649351195285\t Accuracy: 0.9163240131578947\n",
            "Epoch [430/500]\t Loss: 0.2649105073590028\t Accuracy: 0.9171463815789473\n",
            "Epoch [440/500]\t Loss: 0.2623133957386017\t Accuracy: 0.9181743421052632\n",
            "Epoch [450/500]\t Loss: 0.259771267050191\t Accuracy: 0.9185855263157895\n",
            "Epoch [460/500]\t Loss: 0.2572818453374662\t Accuracy: 0.9198190789473685\n",
            "Epoch [470/500]\t Loss: 0.25484309070988703\t Accuracy: 0.9208470394736842\n",
            "Epoch [480/500]\t Loss: 0.2524530095489402\t Accuracy: 0.9220805921052632\n",
            "Epoch [490/500]\t Loss: 0.25010978077587326\t Accuracy: 0.9229029605263158\n",
            "[FINAL]\t Loss: 0.5378814753024809\t Accuracy: 0.8247227822580645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxK5MuRbR7tW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}